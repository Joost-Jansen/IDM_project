{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in Terminal to create a virtual environment for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make virtual environment. Tested on python3.6, python3.7. Run in terminal, not in Jupyter. TODO: Python3.8+ requires Tensorflow >=2 migration\n",
    "python3.7 -m venv tutorial-env\n",
    "source tutorial-env/bin/activate\n",
    "\n",
    "# Install requirements\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "# point jupyter to virtualenv \n",
    "pip3 install ipykernel\n",
    "python -m ipykernel install --user --name=tutorial_env\n",
    "\n",
    "# Now, restart jupyter notebook and change the Kernel by selecting Kernel > Change kernel > tutorial_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__) # should be 1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first tutorial, we choose to run an experiment where we increase the size of the dataset to see how sample efficient the methods are. The environment is the Graph environment with a horizon of 4 and a tabular function class for the Q functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "from ope.envs.graph import Graph\n",
    "from ope.models.basics import BasicPolicy\n",
    "\n",
    "from ope.experiment_tools.experiment import ExperimentRunner, analysis\n",
    "from ope.experiment_tools.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner() # Instantiate a runner for an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 5 experiments, each with a varying number of trajectories\n",
    "for N in range(5):\n",
    "\n",
    "    # basic configuration with varying number of trajectories\n",
    "    configuration = {\n",
    "        \"gamma\": 0.98,                # discount factor\n",
    "        \"horizon\": 4,                 # horizon of the environment\n",
    "        \"base_policy\": .8,            # \\pi_b(a = 0)\n",
    "        \"eval_policy\": .2,            # \\pi_e(a = 0)\n",
    "        \"stochastic_env\": True,       # Make environment have stochastic transitions\n",
    "        \"stochastic_rewards\": False,  # Make environment have stochastic rewards\n",
    "        \"sparse_rewards\": False,      # Make environment have sparse rewards\n",
    "        \"num_traj\": 8*2**N,           # Number of trajectories to collect from pi_b\n",
    "        \"is_pomdp\": False,            # Make environment POMDP\n",
    "        \"pomdp_horizon\": 2,           # Horizon of POMDP if is_pomdp is True\n",
    "        \"seed\": 1000,                 # Seed\n",
    "        \"modeltype\": \"tabular\",       # Q function model type\n",
    "        \"to_regress_pi_b\": False,     # pi_b unknown?\n",
    "    }\n",
    "\n",
    "    # store these credentials in an object\n",
    "    cfg = Config(configuration)\n",
    "\n",
    "    # initialize environment with the parameters from the config file.\n",
    "    # If you'd like to use a different environment, swap this line\n",
    "    env = Graph(make_pomdp=cfg.is_pomdp,\n",
    "                number_of_pomdp_states=cfg.pomdp_horizon,\n",
    "                transitions_deterministic=not cfg.stochastic_env,\n",
    "                max_length=cfg.horizon,\n",
    "                sparse_rewards=cfg.sparse_rewards,\n",
    "                stochastic_rewards=cfg.stochastic_rewards)\n",
    "\n",
    "    # set seed for the experiment\n",
    "    np.random.seed(cfg.seed)\n",
    "\n",
    "    # processor processes the state for storage,  {(processor(x), a, r, processor(x'), done)}\n",
    "    processor = lambda x: x\n",
    "\n",
    "    # absorbing state for padding if episode ends before horizon is reached. This is environment dependent.\n",
    "    absorbing_state = processor(np.array([env.n_dim - 1]))\n",
    "\n",
    "    # Setup policies. BasicPolicy takes the form [P(a=0), P(a=1), ..., P(a=n)]\n",
    "    # For different policies, swap in here\n",
    "    actions = [0, 1]\n",
    "    pi_e = BasicPolicy(\n",
    "        actions, [max(.001, cfg.eval_policy), 1 - max(.001, cfg.eval_policy)])\n",
    "    pi_b = BasicPolicy(\n",
    "        actions, [max(.001, cfg.base_policy), 1 - max(.001, cfg.base_policy)])\n",
    "\n",
    "    # add env, policies, absorbing state and processor\n",
    "    cfg.add({\n",
    "        'env': env,\n",
    "        'pi_e': pi_e,\n",
    "        'pi_b': pi_b,\n",
    "        'processor': processor,\n",
    "        'absorbing_state': absorbing_state\n",
    "    })\n",
    "\n",
    "    # Decide which OPE methods to run.\n",
    "    # Currently only all is available\n",
    "    cfg.add({'models': 'all'})\n",
    "\n",
    "    # Add the configuration\n",
    "    runner.add(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "results = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "# Each row in the result is (OPE estimator, V(pi_e), MSE Error from on-policy: (V(pi_e) - True)**2)\n",
    "for num, result in enumerate(results):\n",
    "    print('Result Experiment %s' % (num+1))\n",
    "    analysis(result)\n",
    "    print('*'*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second tutorial, we choose to run the same experiment as before but with a different environment and different Q function class. The environment is the Pixel-Gridworld (Pix-GW) environment with a horizon of 5 and a NN function class for the Q functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ope.envs.gridworld import Gridworld\n",
    "from ope.models.epsilon_greedy_policy import EGreedyPolicy\n",
    "from ope.models.tabular_model import TabularPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner() # make new runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in range(5): # We suggest running this with a GPU or changing this to range(1)\n",
    "    configuration = {\n",
    "        \"gamma\": 0.98,                # discount factor\n",
    "        \"horizon\": 5,                 # horizon of the environment\n",
    "        \"base_policy\": .8,            # Probability of deviation from epsilon-greedy for base policy\n",
    "        \"eval_policy\": .2,            # Probability of deviation from epsilon-greedy for eval policy\n",
    "        \"stochastic_env\": True,       # Make environment have stochastic transitions       \n",
    "        \"stochastic_rewards\": False,  # Make environment have stochastic rewards\n",
    "        \"sparse_rewards\": False,      # Make environment have sparse rewards\n",
    "        \"num_traj\": 8*2**N,           # Number of trajectories to collect from pi_b\n",
    "        \"seed\": 1000,                 # Seed\n",
    "        \"to_regress_pi_b\": False,     # pi_b unknown?\n",
    "        \"frameskip\": 1,               # (x_t, a, r, x_{t+frameskip}). Apply action \"a\" frameskip number of times\n",
    "        \"frameheight\": 1,             # (x_{t:t+frameheight}, a, r, x_{t+1:t+1+frameheight}). State is consider a concatenation of frameheight number of states\n",
    "        \"modeltype\": 'conv',          # Q function model type, Convolutional NN\n",
    "        \"Qmodel\": 'conv1',            # Q function NN definition (TODO: should be moved) \n",
    "    }\n",
    "\n",
    "    # store these credentials in an object\n",
    "    cfg = Config(configuration)\n",
    "\n",
    "    # initialize environment with the parameters from the config file.\n",
    "    env = Gridworld(slippage=.2*cfg.stochastic_env)\n",
    "\n",
    "    # Set seed and \n",
    "    np.random.seed(cfg.seed)\n",
    "    eval_policy = cfg.eval_policy\n",
    "    base_policy = cfg.base_policy\n",
    "\n",
    "    # to_grid and from_grid are particular to Gridworld\n",
    "    # These functions are special to convert an index in a grid to an 'image'\n",
    "    def to_grid(x, gridsize=[8, 8]):\n",
    "        x = x.reshape(-1)\n",
    "        x = x[0]\n",
    "        out = np.zeros(gridsize)\n",
    "        if x >= 64:\n",
    "            return out\n",
    "        else:\n",
    "            out[x//gridsize[0], x%gridsize[1]] = 1.\n",
    "        return out\n",
    "\n",
    "    # This function takes an 'image' and returns the position in the grid\n",
    "    def from_grid(x, gridsize=[8, 8]):\n",
    "        if len(x.shape) == 3:\n",
    "            if np.sum(x) == 0:\n",
    "                x = np.array([gridsize[0] * gridsize[1]])\n",
    "            else:\n",
    "                x = np.array([np.argmax(x.reshape(-1))])\n",
    "        return x\n",
    "\n",
    "    # processor processes the state for storage,  {(processor(x), a, r, processor(x'), done)}\n",
    "    processor = lambda x: x\n",
    "    \n",
    "    # Set up e-greedy policy using epsilon-optimal\n",
    "    policy = env.best_policy()\n",
    "    \n",
    "    # absorbing state for padding if episode ends before horizon is reached. This is environment dependent.\n",
    "    absorbing_state = processor(np.array([len(policy)]))\n",
    "\n",
    "    # Setup policies.\n",
    "    pi_e = EGreedyPolicy(model=TabularPolicy(policy, absorbing=absorbing_state), processor=from_grid, prob_deviation=eval_policy, action_space_dim=env.n_actions)\n",
    "    pi_b = EGreedyPolicy(model=TabularPolicy(policy, absorbing=absorbing_state), processor=from_grid, prob_deviation=base_policy, action_space_dim=env.n_actions)\n",
    "\n",
    "    cfg.add({\n",
    "        'env': env,\n",
    "        'pi_e': pi_e,\n",
    "        'pi_b': pi_b,\n",
    "        'processor': processor,\n",
    "        'absorbing_state': absorbing_state,\n",
    "        'convert_from_int_to_img': to_grid, # if environment state is an int, can convert to image through this function\n",
    "    })\n",
    "    cfg.add({'models': 'all'})\n",
    "\n",
    "    runner.add(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "results = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "# Each row in the result is (OPE estimator, V(pi_e), MSE Error from on-policy: (V(pi_e) - True)**2)\n",
    "for num, result in enumerate(results):\n",
    "    print('Result Experiment %s' % (num+1))\n",
    "    analysis(result)\n",
    "    print('*'*20)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial_env",
   "language": "python",
   "name": "tutorial_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
